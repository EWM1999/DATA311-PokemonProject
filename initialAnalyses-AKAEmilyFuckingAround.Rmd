---
title: "Project"
author: "Emily Medema"
date: '2019-03-06'
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Project: Pokemon Dataset

```{r}
pokemon<-read.csv("pokemon_alopez247.csv")
#plot(pokemon)
#plot(pokemon$Name~pokemon$Type_1)
#plot(pokemon$Name~. , data = pokemon, col = "blueviolet")
#look at all those tasty graphs
summary(pokemon)

```

Let's try fitting a linear model with the response variable total and predictors HP, Attack, and Defense.

```{r}
linmod <- lm(pokemon$Total~pokemon$HP+pokemon$Attack+pokemon$Defense)
summary(linmod)
#plot(linmod)
plot(pokemon$HP+pokemon$Attack+pokemon$Defense, pokemon$Total)
abline(linmod, h = 0.5, col = "red")
#mmmm tasty sig values
```


First attempt at Clustering
```{r}
eucdist<-dist(pokemon, method="euclidean")
clusPokemon<-hclust(eucdist, method = "single")
plot(clusPokemon)
clusPokemonAvg<-hclust(eucdist, method = "average")
plot(clusPokemonAvg)
clusComplete<-hclust(eucdist, method = "complete")
plot(clusComplete)
```

@Barrett

See what a regression tree looks like using total as the predictor and hp, attack, defense, sp_atk, sp_def, and speed as predictors.
```{r}
library(tree)
poke<-data.frame(pokemon)
attach(poke)
pocl<-tree(Total~HP+Attack+Defense+Sp_Atk+Sp_Def+Speed,data=poke)
plot(pocl)
text(pocl)
```

Now let's try pruning it back

```{r}
cv.pocl<-cv.tree(pocl, FUN=prune.tree)
plot(cv.pocl,type="b")
p.pocl<-prune.tree(pocl,best=10)
plot(p.pocl)
text(p.pocl)
summary(p.pocl)
```

Looks like pruning was unnecessary since the lowest MSE is with 12 nodes...

How about with bagging...

```{r}
library(randomForest)
set.seed(1995)
pokebag<-randomForest(Total~HP+Attack+Defense+Sp_Atk+Sp_Def+Speed,data=poke,mtry=6,importance=FALSE)
pokebag
```
Random forest where m=3
```{r}
pokeRF<-randomForest(Total~HP+Attack+Defense+Sp_Atk+Sp_Def+Speed,data=poke,mtry=3,importance=TRUE)
pokeRF
```


Before we get too far along, let's split up the data into training and testing sets
```{r}
#n <- nrow(pokemon)
#shuffled_df <- pokemon[sample(n), ]
#train_indices <- 1:round(0.6 * n)
#train <- shuffled_df[train_indices, ]
#test_indices <- (round(0.6 * n) + 1):n
#test <- shuffled_df[test_indices, ]
set.seed(1995)
train<-sample(1:nrow(poke),432)
poke.test<-pokemon[-train,]
poke.train<-pokemon[train,]

```


Now, let's try to predict if a pokemon is legendary. I would assume that pokemon with a high total are legendary and after taking a peek at the data, it looks like most legendary pokemon do not have a gender. Let's see if this holds true in general.

```{r}
#https://www.kaggle.com/excaliburzero/predicting-legendary-pokemon
maxTotal<-order(poke.train$Total, decreasing = TRUE)
head(poke.train[maxTotal,])
```

This looks like if we have a high total the pokemon is most likely legendary. Let's graph it to see if it holds true

```{r}
library(ggplot2)
plot<-ggplot(poke.train, aes(x =Total, fill = isLegendary)) + geom_histogram()
plot
```

From this graph we can see that the higher the total the more likely a pokemon is to be legendary. In fact, it appears that a pokemon is only legendary when it is above 650 in total and most likely legendary from around 550-625

Now let's look at gender correlation

```{r}
poke.train$hasGender<-factor(poke.train$hasGender)
plot2<-ggplot(poke.train, aes(x =hasGender, fill = isLegendary)) + geom_bar()
plot2
```

This plot shows that most legendary pokemon do not have a gender.


