---
title: "Pokemon"
author: "Barret Jackson"
date: "March 11, 2019"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
See what a regression tree looks like using total as the predictor and hp, attack, defense, sp_atk, sp_def, and speed as predictors.
```{r}
pokemon<-read.csv("pokemon_alopez247.csv")
library(tree)
poke<-data.frame(pokemon)
attach(poke)
pocl<-tree(Total~HP+Attack+Defense+Sp_Atk+Sp_Def+Speed,data=poke)
plot(pocl)
text(pocl)
```

Now let's try pruning it back

```{r}
cv.pocl<-cv.tree(pocl, FUN=prune.tree)
plot(cv.pocl,type="b")
p.pocl<-prune.tree(pocl,best=10)
plot(p.pocl)
text(p.pocl)
summary(p.pocl)
```

Looks like pruning was unnecessary since the lowest MSE is with 12 nodes...

How about with bagging...

```{r}
library(randomForest)
set.seed(1995)
pokebag<-randomForest(Total~HP+Attack+Defense+Sp_Atk+Sp_Def+Speed,data=poke,mtry=6,importance=FALSE)
pokebag
```
Random forest where m=3
```{r}
pokeRF<-randomForest(Total~HP+Attack+Defense+Sp_Atk+Sp_Def+Speed,data=poke,mtry=3,importance=TRUE)
pokeRF
```
Now for a classification tree. Check out isLegendary as the response from the previous predictors.

```{r}
pokeclass<-tree(isLegendary~HP+Attack+Defense+Sp_Atk+Sp_Def+Speed,data=poke)
plot(pokeclass)
text(pokeclass,pretty=0)
```

Now let's cross-validate.
```{r}
set.seed(1995)
cv.pokeclass<-cv.tree(pokeclass, FUN=prune.misclass)
plot(cv.pokeclass,type="b")
```
Cross-validation shows 4 terminal nodes makes for the best classifier
```{r}
p.pokeclass<-prune.misclass(pokeclass,best=4)
plot(p.pokeclass)
text(p.pokeclass, pretty=0)
summary(p.pokeclass)
cv.pokeclass
pokeclass.pred<-predict(p.pokeclass,type="class")

table(isLegendary,pokeclass.pred)
```


Now let's try using bagging on the model.
```{r}
library(randomForest)
set.seed(1995)
bag.pokeclass<-randomForest(isLegendary~HP+Attack+Defense+Sp_Atk+Sp_Def+Speed,data=poke,mtry=6,importance=TRUE)
bag.pokeclass
importance(bag.pokeclass)
varImpPlot(bag.pokeclass)

```

Looks like lots of false positives with this model. Let's try using random forest with m=3

```{r}
rf.pokeclass<-randomForest(isLegendary~HP+Attack+Defense+Sp_Atk+Sp_Def+Speed,data=poke,mtry=3,importance=TRUE)
rf.pokeclass
importance(rf.pokeclass)
varImpPlot(rf.pokeclass)
```

Even worse for false positives using random forest.

My training/testing set
```{r}
set.seed(1995)
train<-sample(1:nrow(poke),432)
poke.test<-poke[-train,]
poke.train<-poke[train,]
```
I'm going to create a tree using a training data set and see how it performs against testing data.
```{r}
tree.pokemon<-tree(isLegendary~HP+Attack+Defense+Sp_Atk+Sp_Def+Speed,data=poke.train)
tree.pred<-predict(tree.pokemon,poke.test,type="class")
table(tree.pred,poke.test$isLegendary)
```


Emily's training/testing set
```{r}
n <- nrow(pokemon)
shuffled_df <- pokemon[sample(n), ]
train_indices <- 1:round(0.6 * n)
train <- shuffled_df[train_indices, ]
test_indices <- (round(0.6 * n) + 1):n
test <- shuffled_df[test_indices, ]
train
test
```

##LDA/QDA on isLegendary

Let's try out LDA

```{r}
library(MASS)
lda.poke<-lda(isLegendary~HP+Attack+Defense+Sp_Atk+Sp_Def+Speed,data=poke.train)
lda.poke
```

Looking good, let's see how it does on prediction.
```{r}
lda.pred<-predict(lda.poke,poke.test$isLegendary)
table(isLegendary,lda.pred$class)
```

Alright, qda now.
```{r}
qda.poke<-qda(isLegendary~HP+Attack+Defense+Sp_Atk+Sp_Def+Speed,data=poke.train)
qda.poke

```

Exactly the same as lda...

I think this next bit is logreg?

```{r}
log.poke <- glm(isLegendary~HP+Attack+Defense+Sp_Atk+Sp_Def+Speed , family="binomial")
table(predict(log.poke, type="response") > 0.5, isLegendary)
summary(log.poke)
```

Holy shit, log reg gives the best predictions here.

I'm gonna try calculating logloss now
```{r}
LogLoss(predict(lda.poke)$posterior[,2], as.numeric(poke.train$isLegendary)-1)
LogLoss(predict(qda.poke)$posterior[,2], as.numeric(poke.train$isLegendary)-1)
LogLoss(predict(log.poke), as.numeric(isLegendary)-1)

```

Logreg had the highest log loss, if I'm calculating this correctly(?).