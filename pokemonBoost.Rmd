---
title: "Boosting"
author: "Kat"
date: '2019-03-19'
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
pokemon<-read.csv("data/pokemon_alopez247.csv")
poke <- data.frame(pokemon)

# change isLegendary to numeric category
poke$isLegendary<-(as.integer(factor(poke$isLegendary))-1)
poke<-poke[,-c(1,2)]

# split into training and testing dagta sets
set.seed(1995)
train<-sample(1:nrow(poke),432)
poke.test<-poke[-train,]
poke.train<-poke[train,]
```

```{r}
library(glmnet)
library(gbm)
library(gclus)
```

## Boosting

Let's try using boosting on the pokemon data to predict if Legendary
```{r}

 set.seed(54390)
 pboost <- gbm(isLegendary~., distribution="bernoulli", data=poke, n.trees=5000, interaction.depth=1)
 table(poke$isLegendary, predict(pboost, newdata=poke, type="response", n.trees=5000)>0.5 )
 summary(pboost)
```


looks like it might be over fitting. Let's try splitting the data up into training and testing data....
```{r}
set.seed(54390)
pboost <- gbm(isLegendary~., distribution="bernoulli", data=poke.train, n.trees=5000, interaction.depth=1)
table(poke.test$isLegendary, predict(pboost, newdata=poke.test, type="response", n.trees=5000)>0.5 )
summary(pboost)
```

